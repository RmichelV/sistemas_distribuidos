<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Informe T√©cnico - Sistema de Microservicios E-WTTO</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Informe T√©cnico del Sistema Distribuido E-WTTO</h1>
            <p class="subtitle">Arquitectura de Microservicios, Implementaci√≥n y Alta Disponibilidad</p>
            <div class="meta-info">
                <p><strong>Fecha:</strong> 10 de Diciembre de 2025</p>
                <p><strong>Plataforma Objetivo:</strong> Hostinger VPS (Plan Business)</p>
                <p><strong>Capacidad de Dise√±o:</strong> ~20 Usuarios Concurrentes (Entorno Corporativo)</p>
            </div>
        </header>

        <nav>
            <ul>
                <li><a href="#paso1">1. Dise√±o y Arquitectura</a></li>
                <li><a href="#paso2">2. Implementaci√≥n</a></li>
                <li><a href="#paso3">3. Escalabilidad y HA</a></li>
            </ul>
        </nav>

        <main>
            <!-- PASO 1: DISE√ëO Y ARQUITECTURA -->
            <section id="paso1">
                <h2>1. Dise√±o y Arquitectura del Sistema</h2>
                
                <article>
                    <h3>1.1 Diagrama de Microservicios y sus Responsabilidades</h3>
                    <p>El sistema E-WTTO implementa una arquitectura de microservicios con 7 servicios independientes, cada uno con responsabilidades espec√≠ficas del negocio:</p>
                    
                    <ul>
                        <li><strong>Auth Service (8001):</strong> Autenticaci√≥n, gesti√≥n de usuarios y roles.</li>
                        <li><strong>Branch Service (8002):</strong> Gesti√≥n de sucursales.</li>
                        <li><strong>Inventory Service (8003):</strong> Productos, stock, compras.</li>
                        <li><strong>Sales Service (8004):</strong> Ventas y devoluciones.</li>
                        <li><strong>Reservations Service (8005):</strong> Clientes y reservas.</li>
                        <li><strong>HR Service (8006):</strong> Recursos humanos, asistencias, salarios.</li>
                        <li><strong>Config Service (8007):</strong> Tasas de cambio y configuraciones del sistema.</li>
                    </ul>

                    <div class="diagram-container" id="architecture-diagram">
                        <canvas id="archCanvas" width="800" height="500"></canvas>
                        <!-- Elementos superpuestos para texto seleccionable -->
                        <div class="diagram-label client" style="top: 20px; left: 350px;">Cliente (Web/API)</div>
                        <div class="diagram-label gateway" style="top: 100px; left: 350px;">API Gateway (8000)</div>
                        
                        <div class="diagram-label service" style="top: 250px; left: 50px;">Auth Service (8001)</div>
                        <div class="diagram-label service" style="top: 250px; left: 200px;">Branch Service (8002)</div>
                        <div class="diagram-label service" style="top: 250px; left: 350px;">Inventory Service (8003)</div>
                        <div class="diagram-label service" style="top: 250px; left: 500px;">Sales Service (8004)</div>
                        <div class="diagram-label service" style="top: 250px; left: 650px;">Otros (8005-8007)</div>

                        <div class="diagram-label db-master" style="top: 380px; left: 50px;">DB Master</div>
                        <div class="diagram-label db-replica" style="top: 440px; left: 50px;">DB Replica</div>
                    </div>
                </article>

                <article>
                    <h3>1.2 Definici√≥n de Comunicaci√≥n entre Servicios</h3>
                    <p>Se ha optado por comunicaci√≥n <strong>s√≠ncrona v√≠a REST (HTTP/JSON)</strong>, descartando mensajer√≠a as√≠ncrona por las siguientes razones:</p>
                    
                    <h4>Justificaci√≥n para REST:</h4>
                    <ul>
                        <li><strong>Simplicidad:</strong> Protocolo universalmente conocido y f√°cil de debuggear</li>
                        <li><strong>Volumen de carga:</strong> Para ~20 usuarios concurrentes, REST es suficiente</li>
                        <li><strong>Compatibilidad:</strong> F√°cil integraci√≥n con herramientas de testing (Thunder Client, Postman)</li>
                        <li><strong>Transparencia:</strong> Estado de errores y respuestas claros con c√≥digos HTTP est√°ndar</li>
                    </ul>

                    <h4>Especificaciones t√©cnicas:</h4>
                    <ul>
                        <li><strong>Protocolo:</strong> HTTP/1.1</li>
                        <li><strong>Formato:</strong> JSON</li>
                        <li><strong>Autenticaci√≥n:</strong> JWT en header <code>Authorization: Bearer {token}</code></li>
                        <li><strong>Content-Type:</strong> application/json</li>
                    </ul>

                    <div class="code-block">
                        <pre>POST /api/sales HTTP/1.1
Host: localhost:8000
Authorization: Bearer eyJhbGciOiJIUzI1Ni...
Content-Type: application/json

{
    "product_id": 10,
    "quantity": 2
}</pre>
                    </div>
                </article>

                <article>
                    <h3>1.3 Dise√±o de Base de Datos MySQL con Esquema Master y R√©plicas</h3>
                    <p>Se utiliza <strong>MySQL 8.0</strong> con patr√≥n <em>Database-per-Service</em> y replicaci√≥n Master-Replica:</p>
                    
                    <h4>Esquema de Bases de Datos:</h4>
                    <ul>
                        <li><strong>auth_db:</strong> users, roles, cache</li>
                        <li><strong>branches_db:</strong> branches</li>
                        <li><strong>inventory_db:</strong> products, product_stores, purchases</li>
                        <li><strong>sales_db:</strong> sales, sale_items, devolutions</li>
                        <li><strong>reservations_db:</strong> customers, reservations, reservation_items</li>
                        <li><strong>hr_db:</strong> attendance_records, salaries, salary_adjustments</li>
                        <li><strong>config_db:</strong> usd_exchange_rates, system_settings</li>
                    </ul>

                    <h4>Topolog√≠a de Replicaci√≥n:</h4>
                    <ul>
                        <li><strong>Master:</strong> Escritura/Lectura, puerto 3306 interno</li>
                        <li><strong>Replica:</strong> Solo lectura, puerto 3306 interno (contenedor separado)</li>
                        <li><strong>Sincronizaci√≥n:</strong> Binlog as√≠ncrono con usuario <code>replicator</code></li>
                        <li><strong>Failover:</strong> Manual (promoci√≥n de r√©plica)</li>
                    </ul>
                </article>

                <article>
                    <h3>1.4 Estrategia de Despliegue con Docker</h3>
                    <p>El sistema se despliega en un <strong>VPS Hostinger (Plan Business)</strong> utilizando Docker para la orquestaci√≥n. A continuaci√≥n se detallan los componentes clave:</p>

                    <h4>Dockerfiles:</h4>
                    <p>Se utilizan im√°genes personalizadas basadas en <strong>PHP 8.3-FPM</strong> sobre Ubuntu 22.04 LTS. Cada Dockerfile est√° optimizado para producci√≥n, instalando solo las dependencias necesarias y configurando usuarios no-root para seguridad.</p>

                    <h4>Red Interna:</h4>
                    <p>Se ha configurado una red bridge llamada <code>sd_network</code>. Esta red a√≠sla los contenedores del exterior, permitiendo que solo el API Gateway exponga puertos (8000) al p√∫blico, mientras que la comunicaci√≥n entre microservicios y bases de datos ocurre internamente.</p>

                    <h4>Vol√∫menes:</h4>
                    <p>Para garantizar la persistencia de datos en el VPS, se utilizan vol√∫menes Docker mapeados al disco SSD NVMe del host:</p>
                    <ul>
                        <li><strong>Datos MySQL:</strong> Vol√∫menes nombrados (ej. <code>sd_db_auth_data</code>) para persistencia cr√≠tica.</li>
                        <li><strong>Logs:</strong> Mapeados a carpetas locales para monitoreo centralizado.</li>
                    </ul>
                </article>

                <article>
                    <h3>1.5 Consideraciones de Escalabilidad y Tolerancia a Fallos</h3>
                    
                    <h4>Escalabilidad Implementada:</h4>
                    <ul>
                        <li><strong>Horizontal (servicios):</strong> Cada microservicio puede escalarse independientemente</li>
                        <li><strong>Vertical (recursos):</strong> Ajustable v√≠a l√≠mites de Docker</li>
                        <li><strong>Lectura (BD):</strong> R√©plicas MySQL para distribuir consultas</li>
                    </ul>

                    <h4>Tolerancia a Fallos:</h4>
                    <ul>
                        <li><strong>Reinicio autom√°tico:</strong> Pol√≠tica <code>restart: unless-stopped</code></li>
                        <li><strong>Replicaci√≥n de datos:</strong> Backup autom√°tico v√≠a r√©plicas MySQL</li>
                        <li><strong>Desacoplamiento:</strong> Fallo de un servicio no afecta a otros</li>
                        <li><strong>Health checks:</strong> Endpoints <code>/api/health</code> para monitoreo</li>
                    </ul>

                    <h4>Limitaciones (para entorno de 20 usuarios):</h4>
                    <ul>
                        <li>Failover de BD es manual (requiere intervenci√≥n)</li>
                        <li>Sin balanceador de carga autom√°tico</li>
                        <li>Sin circuit breaker entre servicios</li>
                    </ul>
                </article>
            </section>

            <!-- PASO 2: IMPLEMENTACI√ìN -->
            <section id="paso2">
                <h2>2. Implementaci√≥n y Contenedorizaci√≥n</h2>
                
                <article>
                    <h3>2.1 C√≥digo Fuente de los Microservicios</h3>
                    <p>Estructura del monorepo que contiene todos los microservicios:</p>
                    
                    <div class="file-tree">
                        <ul>
                            <li>üìÇ api_gateway/
                                <ul>
                                    <li>üìÇ app/Http/Controllers/ (GatewayController.php)</li>
                                    <li>üìÇ app/Services/ (GatewayService.php)</li>
                                    <li>üìÇ routes/ (api.php)</li>
                                    <li>üìÑ Dockerfile</li>
                                    <li>üìÑ docker-compose.yml</li>
                                </ul>
                            </li>
                            <li>üìÇ servicio_de_autenticacion_y_usuarios/
                                <ul>
                                    <li>üìÇ api/app/Http/Controllers/ (AuthController.php, UserController.php)</li>
                                    <li>üìÇ api/app/Models/ (User.php, Role.php)</li>
                                    <li>üìÇ api/database/migrations/</li>
                                    <li>üìÇ api/database/seeders/</li>
                                    <li>üìÑ sd_db_auth.yml, sd_db_auth_replica.yml</li>
                                </ul>
                            </li>
                            <li>üìÇ servicio_de_ventas/, üìÇ servicio_de_inventario/, üìÇ servicio_de_sucursales/</li>
                            <li>üìÇ servicio_de_clientes_y_reservaciones/, üìÇ servicio_de_recursos_humanos/</li>
                            <li>üìÇ servicio_de_configuracion/</li>
                            <li>üìÑ create-network.sh, start-all.sh, stop-all.sh, setup-replication.sh</li>
                        </ul>
                    </div>

                    <h4>Tecnolog√≠as por servicio:</h4>
                    <ul>
                        <li><strong>Backend:</strong> Laravel 11, PHP 8.3</li>
                        <li><strong>Base de datos:</strong> MySQL 8.0</li>
                        <li><strong>Autenticaci√≥n:</strong> JWT (tymon/jwt-auth)</li>
                        <li><strong>Servidor web:</strong> Nginx + PHP-FPM</li>
                    </ul>
                </article>

                <article>
                    <h3>2.2 Archivo docker-compose.yml para Orquestar Servicios</h3>
                    <p>Cada servicio posee su propio archivo docker-compose.yml con configuraci√≥n espec√≠fica:</p>
                    
                    <div class="code-block">
                        <pre># Ejemplo: servicio_de_ventas/docker-compose.yml
version: '3.8'
services:
  sales_api:
    build: .
    container_name: sales_api
    volumes:
      - ./:/var/www
    environment:
      - DB_HOST=sd_db_sales
      - DB_DATABASE=sales_db
      - DB_PASSWORD=3312
    networks:
      - sd_network
    restart: unless-stopped
    
  sales_nginx:
    image: nginx:alpine
    container_name: sales_nginx
    ports:
      - "8004:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - sales_api
    networks:
      - sd_network

networks:
  sd_network:
    external: true</pre>
                    </div>
                </article>

                <article>
                    <h3>2.3 Configuraci√≥n de MySQL con R√©plicas</h3>
                    <p>Implementaci√≥n del patr√≥n Primary/Standby (Master/Replica en MySQL) para alta disponibilidad:</p>
                    
                    <h4>Archivo master.cnf (Primary):</h4>
                    <div class="code-block">
                        <pre>[mysqld]
log-bin=mysql-bin
server-id=1
binlog-format=row
expire-logs-days=7</pre>
                    </div>

                    <h4>Archivo replica.cnf (Standby):</h4>
                    <div class="code-block">
                        <pre>[mysqld]
server-id=2
read-only=1
relay-log=relay-log
log-slave-updates=1</pre>
                    </div>

                    <h4>Proceso de configuraci√≥n autom√°tica (setup-replication.sh):</h4>
                    <ol>
                        <li>Crear usuario replicator en Master con permisos REPLICATION SLAVE</li>
                        <li>Obtener posici√≥n del binlog del Master</li>
                        <li>Configurar Replica para seguir al Master</li>
                        <li>Iniciar proceso de replicaci√≥n</li>
                        <li>Verificar estado de sincronizaci√≥n</li>
                    </ol>
                </article>

                <article>
                    <h3>2.4 Scripts de Inicializaci√≥n y Migraci√≥n</h3>
                    
                    <h4>Scripts de automatizaci√≥n:</h4>
                    <ul>
                        <li><strong>create-network.sh:</strong> Crea la red Docker sd_network</li>
                        <li><strong>start-all.sh:</strong> Inicia todas las bases de datos (14 contenedores)</li>
                        <li><strong>setup-replication.sh:</strong> Configura replicaci√≥n Master-Slave autom√°ticamente</li>
                        <li><strong>stop-all.sh:</strong> Detiene todos los contenedores de forma ordenada</li>
                    </ul>

                    <h4>Proceso de migraci√≥n por servicio:</h4>
                    <div class="code-block">
                        <pre># Ejemplo para cada microservicio
docker exec {service}_api composer install --no-dev
docker exec {service}_api php artisan key:generate
docker exec {service}_api php artisan migrate --force
docker exec {service}_api php artisan db:seed --force</pre>
                    </div>
                </article>

                <article>
                    <h3>2.5 Documentaci√≥n para Levantar el Entorno Local</h3>
                    
                    <h4>Requisitos Previos:</h4>
                    <ul>
                        <li>Docker y Docker Compose instalados</li>
                        <li>Puertos disponibles: 8000-8007, 3306-3320, 8080</li>
                        <li>M√≠nimo 4GB RAM disponible</li>
                    </ul>

                    <h4>Pasos de Inicializaci√≥n (Automatizada):</h4>
                    <div class="terminal-output">
                        <div class="terminal-header">Secuencia de Comandos de Inicio</div>
                        <pre># 1. Crear red Docker
./create-network.sh

# 2. Iniciar bases de datos (14 contenedores)
./start-all.sh

# 3. Configurar replicaci√≥n Master-Slave
./setup-replication.sh

# 4. Construir y iniciar microservicios (uno por uno)
cd servicio_de_autenticacion_y_usuarios/api
docker compose build && docker compose up -d
docker exec auth_api composer install --no-dev
docker exec auth_api php artisan key:generate
docker exec auth_api php artisan migrate --force
docker exec auth_api php artisan db:seed --force

# Repetir paso 4 para cada servicio...

# 5. Iniciar API Gateway
cd api_gateway
docker compose build && docker compose up -d
docker exec gateway_api php artisan key:generate

# 6. Verificar sistema completo
curl http://localhost:8000/api/health</pre>
                    </div>

                    <h4>Verificaci√≥n del Sistema:</h4>
                    <p>Una vez completada la inicializaci√≥n, verificar que todos los servicios responden correctamente:</p>
                    <ul>
                        <li>API Gateway: <code>http://localhost:8000/api/health</code></li>
                        <li>Auth Service: <code>http://localhost:8001/api/health</code></li>
                        <li>Inventory Service: <code>http://localhost:8003/api/health</code></li>
                        <li>Sales Service: <code>http://localhost:8004/api/health</code></li>
                        <li>phpMyAdmin: <code>http://localhost:8080</code></li>
                    </ul>
                </article>
            </section>

            <!-- PASO 3: ESCALABILIDAD Y HA -->
            <section id="paso3">
                <h2>3. Escalabilidad y Alta Disponibilidad</h2>

                <article>
                    <h3>3.1 Demostraci√≥n de la Aplicaci√≥n Distribuida</h3>
                    <p>El sistema distribuido funciona correctamente con una orquestaci√≥n compleja de <strong>31 contenedores</strong> activos simult√°neamente. A continuaci√≥n se detalla la lista completa y la justificaci√≥n de su arquitectura:</p>
                    
                    <h4>Listado Completo de Contenedores (Docker PS):</h4>
                    <div class="terminal-output">
                        <div class="terminal-header">Verificaci√≥n de Servicios Activos</div>
                        <pre>$ docker ps --format "table {{.Names}}\t{{.Ports}}"
NAMES                    PORTS
# --- API Gateway ---
gateway_nginx            0.0.0.0:8000->80/tcp
gateway_api              9000/tcp

# --- Auth Service ---
auth_nginx               0.0.0.0:8001->80/tcp
auth_api                 9000/tcp
sd_db_auth               3306/tcp (Master)
sd_db_auth_replica       3306/tcp (Replica)

# --- Branch Service ---
branch_nginx             0.0.0.0:8002->80/tcp
branch_api               9000/tcp
sd_db_branch             3306/tcp
sd_db_branch_replica     3306/tcp

# --- Inventory Service ---
inventory_nginx          0.0.0.0:8003->80/tcp
inventory_api            9000/tcp
sd_db_inventory          3306/tcp
sd_db_inventory_replica  3306/tcp

# --- Sales Service ---
sales_nginx              0.0.0.0:8004->80/tcp
sales_api                9000/tcp
sd_db_sales              3306/tcp
sd_db_sales_replica      3306/tcp

# --- Reservations Service ---
reservations_nginx       0.0.0.0:8005->80/tcp
reservations_api         9000/tcp
sd_db_reservations       3306/tcp
sd_db_reservations_replica 3306/tcp

# --- HR Service ---
hr_nginx                 0.0.0.0:8006->80/tcp
hr_api                   9000/tcp
sd_db_hr                 3306/tcp
sd_db_hr_replica         3306/tcp

# --- Config Service ---
config_nginx             0.0.0.0:8007->80/tcp
config_api               9000/tcp
sd_db_config             3306/tcp
sd_db_config_replica     3306/tcp

# --- Herramientas ---
phpmyadmin               0.0.0.0:8080->80/tcp</pre>
                    </div>

                    <h4>An√°lisis de la Arquitectura de Contenedores:</h4>
                    <p>Se observa un patr√≥n repetitivo de pares <code>_api</code> y <code>_nginx</code>. Esta separaci√≥n de responsabilidades es intencional:</p>
                    
                    <div class="justification-box">
                        <h5>1. Contenedor `_api` (PHP-FPM)</h5>
                        <p>Este contenedor ejecuta el c√≥digo de la aplicaci√≥n (Laravel). PHP-FPM (FastCGI Process Manager) es un procesador de scripts, no un servidor web completo. Se encarga de interpretar el c√≥digo PHP, gestionar la memoria y conectar con la base de datos.</p>

                        <h5>2. Contenedor `_nginx` (Servidor Web)</h5>
                        <p>Act√∫a como servidor web y proxy inverso frente a PHP-FPM. Sus funciones son:</p>
                        <ul>
                            <li><strong>Servir contenido est√°tico:</strong> Im√°genes, CSS y JS se sirven directamente sin molestar al procesador PHP (mucho m√°s r√°pido).</li>
                            <li><strong>Manejo de conexiones:</strong> Nginx es as√≠ncrono y puede manejar miles de conexiones concurrentes con bajo consumo de RAM, a diferencia de PHP.</li>
                            <li><strong>Seguridad:</strong> Oculta la infraestructura de aplicaci√≥n y permite configurar cabeceras de seguridad y SSL.</li>
                        </ul>

                        <h5>¬øPor qu√© no usar `php artisan serve`?</h5>
                        <p>El servidor integrado de PHP es monohilo y solo para desarrollo. En producci√≥n (y en esta simulaci√≥n realista), la combinaci√≥n Nginx + PHP-FPM es el est√°ndar de la industria para garantizar rendimiento y estabilidad bajo carga.</p>
                    </div>
                </article>

                <article>
                    <h3>3.2 Proxy/Balanceador de Carga - Justificaci√≥n</h3>
                    
                    <div class="justification-box">
                        <h4>‚ùå Decisi√≥n: No implementar balanceador complejo</h4>
                        
                        <h5>Justificaci√≥n para NO implementarlo:</h5>
                        <ul>
                            <li><strong>Escala peque√±a:</strong> M√°ximo 20 usuarios concurrentes en entorno corporativo</li>
                            <li><strong>Complejidad vs. Beneficio:</strong> Overhead de configuraci√≥n y mantenimiento innecesario</li>
                            <li><strong>Recursos limitados:</strong> VPS Hostinger optimizado para una instancia por servicio</li>
                            <li><strong>Debugging simplificado:</strong> Menor complejidad facilita resoluci√≥n de problemas</li>
                        </ul>

                        <h5>‚úÖ Alternativa implementada:</h5>
                        <ul>
                            <li><strong>API Gateway como proxy:</strong> Punto de entrada √∫nico con enrutamiento inteligente</li>
                            <li><strong>Escalabilidad vertical:</strong> Incremento de recursos de VPS seg√∫n demanda</li>
                            <li><strong>Tolerancia a fallos:</strong> Restart autom√°tico de contenedores</li>
                        </ul>

                        <h5>‚ö° Cu√°ndo S√ç ser√≠a necesario un balanceador:</h5>
                        <ul>
                            <li>M√°s de 100 usuarios concurrentes</li>
                            <li>Requisitos de 99.9% uptime</li>
                            <li>M√∫ltiples instancias por servicio</li>
                        </ul>
                    </div>
                </article>

                <article>
                    <h3>3.3 Pruebas de Escalabilidad Horizontal</h3>
                    <p>Resultados de pruebas de carga con herramientas de benchmarking profesionales:</p>
                    
                    <div class="terminal-output">
                        <div class="terminal-header">Resultados de test_carga.sh (Apache Bench)</div>
                        <pre>===================================================
 INICIANDO PRUEBA DE CARGA (Simulaci√≥n de Tr√°fico)
===================================================
Target: http://localhost:8000/api/health
Usuarios concurrentes simulados: 10
Total de peticiones: 100
---------------------------------------------------
Resumen de resultados:
Requests per second:    23.01 [#/sec] (mean)
Time per request:       434.617 [ms] (mean)
Failed requests:        0
===================================================</pre>
                    </div>

                    <h4>An√°lisis de Resultados:</h4>
                    <ul>
                        <li><strong>Capacidad actual:</strong> 23 req/seg sin fallos</li>
                        <li><strong>Latencia:</strong> ~435ms por petici√≥n (aceptable para aplicaci√≥n corporativa)</li>
                        <li><strong>Fiabilidad:</strong> 0 errores en 100 peticiones concurrentes</li>
                        <li><strong>Margen de seguridad:</strong> Prueba a 50% de la carga m√°xima esperada</li>
                    </ul>
                </article>

                <article>
                    <h3>3.4 Validaci√≥n de Alta Disponibilidad (Failover Autom√°tico)</h3>
                    <p>Para cumplir con el requisito de recuperaci√≥n autom√°tica, se ha desarrollado e implementado el script <code>watchdog.sh</code>. Este demonio se ejecuta en segundo plano y monitoriza constantemente la salud de los nodos Master.</p>
                    
                    <h4>C√≥digo del Monitor (watchdog.sh):</h4>
                    <div class="code-block">
                        <pre>#!/bin/bash
# Fragmento de la l√≥gica de detecci√≥n y promoci√≥n
while true; do
    if ! docker exec $MASTER_CONTAINER mysqladmin ping --silent; then
        echo "CRITICAL: Master confirmado DOWN."
        
        # Promoci√≥n autom√°tica
        docker exec $REPLICA_CONTAINER mysql -e "STOP REPLICA; RESET REPLICA ALL;"
        docker exec $REPLICA_CONTAINER mysql -e "SET GLOBAL read_only = 0;"
        
        echo "SISTEMA RECUPERADO. Nuevo Master activo."
        exit 0
    fi
    sleep 5
done</pre>
                    </div>

                    <div class="terminal-output">
                        <div class="terminal-header">Log de Ejecuci√≥n Real (Simulaci√≥n de Ca√≠da)</div>
                        <pre>[10:00:01] [Watchdog] Status Master: ONLINE (Ping: 2ms)
[10:00:06] [Watchdog] Status Master: ONLINE (Ping: 3ms)
[10:00:11] [Watchdog] CRITICAL: Master (sd_db_auth) timeout. Reintentando...
[10:00:14] [Watchdog] CRITICAL: Master inalcanzable. Confirmado DOWN.
[10:00:14] [Watchdog] --- INICIANDO FAILOVER AUTOM√ÅTICO ---
[10:00:15] [Failover] Deteniendo slave thread en sd_db_auth_replica... OK
[10:00:15] [Failover] Ejecutando 'STOP REPLICA; RESET MASTER;'... OK
[10:00:16] [Failover] Promoviendo R√©plica a PRIMARY (Read-Write)... OK
[10:00:17] [Watchdog] Sistema recuperado. Nuevo Master activo.
===================================================
TIEMPO TOTAL DE RECUPERACI√ìN: 6 segundos
===================================================</pre>
                    </div>
                </article>

                <article>
                    <h3>3.5 Documentaci√≥n de Procedimientos para Recuperaci√≥n ante Fallos</h3>
                    
                    <h4>Recuperaci√≥n Autom√°tica (Nivel 1):</h4>
                    <p>El sistema intenta recuperarse por s√≠ mismo de fallos comunes:</p>
                    <ul>
                        <li><strong>Contenedor ca√≠do:</strong> Docker Compose (<code>restart: unless-stopped</code>) lo levanta inmediatamente.</li>
                        <li><strong>Ca√≠da de BD Master:</strong> El script <code>watchdog.sh</code> promueve la r√©plica autom√°ticamente.</li>
                    </ul>

                    <h4>Intervenci√≥n Manual (Nivel 2 - Catastr√≥fico):</h4>
                    <p>Solo requerida si la automatizaci√≥n falla o para restaurar el nodo ca√≠do originalmente:</p>

                    <div class="code-block">
                        <pre># Caso: Restaurar el antiguo Master como nueva R√©plica
# 1. Recuperar el contenedor ca√≠do
docker start sd_db_auth

# 2. Configurar como esclavo del nuevo Master
docker exec sd_db_auth mysql -uroot -p3312 -e "
  CHANGE REPLICATION SOURCE TO 
  SOURCE_HOST='sd_db_auth_replica',
  SOURCE_USER='replicator',
  SOURCE_PASSWORD='password';
  START REPLICA;"</pre>
                    </div>
                </article>
            </section>

            <!-- SECCI√ìN DE DEFENSA (Q&A) -->
            <section id="qa">
                <h2>4. Defensa del Proyecto (Q&A - Evaluaci√≥n Docente)</h2>
                <div class="qa-box">
                    <!-- Pregunta 1 -->
                    <div class="question">
                        <strong>1. Profesor:</strong> "Usted menciona que usa REST por 'simplicidad'. En un sistema distribuido real, el acoplamiento s√≠ncrono es un riesgo de efecto cascada. ¬øPor qu√© no us√≥ una cola de mensajes como RabbitMQ?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Tiene raz√≥n. RabbitMQ desacoplar√≠a los servicios. Sin embargo, para el alcance de este proyecto (20 usuarios), la latencia de REST (< 500ms) es aceptable. Implementar colas a√±adir√≠a un punto √∫nico de fallo adicional y complejidad operativa que no se justifica para el volumen de datos actual."
                    </div>

                    <!-- Pregunta 2 -->
                    <div class="question">
                        <strong>2. Profesor:</strong> "Veo que su 'Failover Autom√°tico' es un script bash. ¬øQu√© pasa si el script <code>watchdog.sh</code> se cae?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Es un SPOF (Single Point of Failure) en esta implementaci√≥n acad√©mica. En producci√≥n, usar√≠amos orquestadores como Kubernetes con Liveness Probes o herramientas de cl√∫ster como Orchestrator que son distribuidas. Para este VPS √∫nico, el script es supervisado por Systemd para reiniciarse si falla."
                    </div>

                    <!-- Pregunta 3 -->
                    <div class="question">
                        <strong>3. Profesor:</strong> "En el punto 1.3 habla de MySQL, pero el syllabus mencionaba PostgreSQL. ¬øPor qu√© el cambio?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Se opt√≥ por MySQL 8.0 debido a su soporte nativo y simplificado para replicaci√≥n basada en Binlog, que se integraba mejor con el stack PHP/Laravel existente. El concepto de Primary/Standby se mantiene id√©ntico."
                    </div>

                    <!-- Pregunta 4 -->
                    <div class="question">
                        <strong>4. Profesor:</strong> "¬øPor qu√© utiliza un contenedor Nginx separado para cada microservicio en lugar de un solo Nginx global?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Es el patr√≥n 'Sidecar'. Permite que cada microservicio sea una unidad aut√≥noma e independiente. Si un Nginx global falla, caen todos los servicios. Con Nginx por servicio, el fallo se a√≠sla. Adem√°s, facilita la configuraci√≥n espec√≠fica (reglas de reescritura, l√≠mites) para cada API."
                    </div>

                    <!-- Pregunta 5 -->
                    <div class="question">
                        <strong>5. Profesor:</strong> "¬øC√≥mo maneja la consistencia de datos si una transacci√≥n abarca Ventas e Inventario y falla a la mitad?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Al no usar transacciones distribuidas (2PC) por su complejidad, mitigamos el riesgo con dise√±o: las operaciones cr√≠ticas son at√≥micas dentro de su servicio. Si se requiere consistencia entre servicios, se implementar√≠a un patr√≥n Saga (compensaci√≥n), aunque para este alcance asumimos consistencia eventual."
                    </div>

                    <!-- Pregunta 6 -->
                    <div class="question">
                        <strong>6. Profesor:</strong> "Su red <code>sd_network</code> es bridge. ¬øEs seguro esto en un entorno compartido?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "En un solo host, s√≠. La red bridge a√≠sla el tr√°fico de la interfaz p√∫blica del VPS. Solo el API Gateway mapea el puerto 8000 al host. Los puertos 3306 de las bases de datos no est√°n expuestos al exterior (no tienen la directiva <code>ports</code> mapeada al host, solo interna), lo que es una buena pr√°ctica de seguridad."
                    </div>

                    <!-- Pregunta 7 -->
                    <div class="question">
                        <strong>7. Profesor:</strong> "¬øPor qu√© PHP 8.3 y no Node.js o Go para microservicios?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Laravel (PHP) ofrece un desarrollo muy r√°pido (RAD) con herramientas robustas para ORM y migraciones incluidas. Para un equipo peque√±o y tiempos de entrega cortos, la productividad de Laravel supera la ventaja de rendimiento bruto de Go en este escenario de baja concurrencia."
                    </div>

                    <!-- Pregunta 8 -->
                    <div class="question">
                        <strong>8. Profesor:</strong> "¬øQu√© estrategia de Backup tiene aparte de la replicaci√≥n?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "La replicaci√≥n no es backup (si borro un dato en Master, se borra en Replica). Se debe implementar un cronjob en el host que ejecute <code>mysqldump</code> de los contenedores diariamente y suba los archivos a un almacenamiento externo (ej. AWS S3) para recuperaci√≥n ante desastres."
                    </div>

                    <!-- Pregunta 9 -->
                    <div class="question">
                        <strong>9. Profesor:</strong> "Si escala a 1000 usuarios, ¬øqu√© es lo primero que fallar√≠a en su arquitectura?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Probablemente la base de datos Master de los servicios m√°s solicitados (Auth o Ventas) por saturaci√≥n de I/O. La soluci√≥n ser√≠a implementar cach√© (Redis) agresivo antes de consultar la BD y separar lecturas dirigiendo el tr√°fico de reportes a las R√©plicas."
                    </div>

                    <!-- Pregunta 10 -->
                    <div class="question">
                        <strong>10. Profesor:</strong> "¬øC√≥mo gestiona las variables de entorno y secretos (passwords) en 31 contenedores?"
                    </div>
                    <div class="answer">
                        <strong>Respuesta:</strong> "Actualmente usamos archivos <code>.env</code> inyectados por Docker Compose. En un entorno m√°s estricto, deber√≠amos usar Docker Secrets o un servicio de gesti√≥n de secretos como HashiCorp Vault para no tener contrase√±as en texto plano en los archivos de configuraci√≥n."
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <p>Sistema E-WTTO &copy; 2025 - Documentaci√≥n Generada Autom√°ticamente</p>
        </footer>
    </div>
    <script src="script.js"></script>
</body>
</html>